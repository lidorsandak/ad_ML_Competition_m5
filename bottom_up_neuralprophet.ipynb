{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom-Up NeuralProphet Submission Generator\n",
    "\n",
    "Streamlined notebook for generating Kaggle submissions using NeuralProphet.\n",
    "\n",
    "**Strategy**: Train individual models for stores 1-10, then sum predictions for Store 0 (bottom-up approach)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: epochs=200, n_lags=84, lr=0.005\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from neuralprophet import NeuralProphet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.data_loader import load_all_data, parse_submission_id\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Adjust these values to tune the model\n",
    "# =============================================================================\n",
    "EPOCHS = 200          # Try: 200, 300, 500 for better convergence\n",
    "N_LAGS = 84           # Try: 56 (8 weeks), 84 (12 weeks) for more history\n",
    "LEARNING_RATE = 0.005  # Try: 0.005, 0.001 for more stable training\n",
    "\n",
    "# Future regressors to use\n",
    "FUTURE_REGRESSORS = [\n",
    "    'is_major_holiday',\n",
    "    'is_sports_event',\n",
    "    'is_religious_event',\n",
    "    'month',\n",
    "    'is_weekend'\n",
    "]\n",
    "\n",
    "print(f\"Configuration: epochs={EPOCHS}, n_lags={N_LAGS}, lr={LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (18766, 4)\n",
      "Date range: 2011-01-29 00:00:00 to 2015-09-30 00:00:00\n",
      "Stores: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Calendar events shape: (162, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df, calendar_df, submission_df = load_all_data()\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Date range: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"Stores: {sorted(train_df['store_id'].unique())}\")\n",
    "print(f\"\\nCalendar events shape: {calendar_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data with features: (18766, 10)\n",
      "Columns: ['store_id', 'store_name', 'date', 'revenue', 'month', 'is_weekend', 'day_of_week', 'is_major_holiday', 'is_sports_event', 'is_religious_event']\n"
     ]
    }
   ],
   "source": [
    "def prepare_calendar_features(df, calendar_df):\n",
    "    \"\"\"\n",
    "    Add calendar event features to dataframe.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "\n",
    "    # Parse calendar dates (DD/MM/YYYY format)\n",
    "    calendar_df = calendar_df.copy()\n",
    "    calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%d/%m/%Y')\n",
    "    \n",
    "    # Define event categories\n",
    "    major_holidays = ['Christmas', 'Thanksgiving', 'NewYear', 'IndependenceDay', \n",
    "                      'MemorialDay', 'LaborDay', 'ValentinesDay', \"Mother's day\", \n",
    "                      \"Father's day\"]\n",
    "    sports_events = ['SuperBowl', 'NBAFinalsStart', 'NBAFinalsEnd']\n",
    "    religious_events = ['Easter', 'OrthodoxEaster', 'LentStart', 'Ramadan starts', \n",
    "                        'Eid al-Fitr', 'Pesach End']\n",
    "    \n",
    "    # Initialize event columns\n",
    "    df['is_major_holiday'] = 0\n",
    "    df['is_sports_event'] = 0\n",
    "    df['is_religious_event'] = 0\n",
    "    \n",
    "    # Mark events\n",
    "    for _, row in calendar_df.iterrows():\n",
    "        event_date = row['date']\n",
    "        events = [e.strip() for e in str(row['event']).split(',')]\n",
    "        \n",
    "        mask = df['date'] == event_date\n",
    "        \n",
    "        for event in events:\n",
    "            if event in major_holidays:\n",
    "                df.loc[mask, 'is_major_holiday'] = 1\n",
    "            if event in sports_events:\n",
    "                df.loc[mask, 'is_sports_event'] = 1\n",
    "            if event in religious_events:\n",
    "                df.loc[mask, 'is_religious_event'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_neuralprophet_data(df, store_id):\n",
    "    \"\"\"\n",
    "    Prepare data for NeuralProphet format.\n",
    "    \"\"\"\n",
    "    store_df = df[df['store_id'] == store_id].copy()\n",
    "    store_df = store_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    prophet_df = pd.DataFrame({\n",
    "        'ds': store_df['date'],\n",
    "        'y': np.log1p(store_df['revenue'])\n",
    "    })\n",
    "    \n",
    "    for col in FUTURE_REGRESSORS:\n",
    "        if col in store_df.columns:\n",
    "            prophet_df[col] = store_df[col].values\n",
    "    \n",
    "    return prophet_df\n",
    "\n",
    "\n",
    "# Apply calendar features to full training data\n",
    "full_train_with_events = prepare_calendar_features(train_df, calendar_df)\n",
    "print(f\"Training data with features: {full_train_with_events.shape}\")\n",
    "print(f\"Columns: {list(full_train_with_events.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - Model Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def train_neuralprophet_model(train_df, store_id, forecast_horizon, add_regressors=True, verbose=False,\n",
    "                               epochs=100, n_lags=28, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Train NeuralProphet model for a specific store.\n",
    "    \"\"\"\n",
    "    df = prepare_neuralprophet_data(train_df, store_id)\n",
    "    \n",
    "    model = NeuralProphet(\n",
    "        growth='linear',\n",
    "        n_changepoints=10,\n",
    "        changepoints_range=0.9,\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        seasonality_mode='additive',\n",
    "        seasonality_reg=0.1,\n",
    "        n_forecasts=forecast_horizon,                    \n",
    "        n_lags=n_lags,\n",
    "        ar_layers=[32, 32],\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        normalize='auto',\n",
    "        drop_missing=True,\n",
    "        trainer_config={\n",
    "            'logger': False,\n",
    "            'enable_progress_bar': verbose,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if add_regressors:\n",
    "        for col in FUTURE_REGRESSORS:\n",
    "            model.add_future_regressor(col)\n",
    "    \n",
    "    model.fit(df, freq='D', validation_df=None, progress=None if not verbose else 'bar')\n",
    "    model.forecast_horizon = forecast_horizon\n",
    "    \n",
    "    return model, df\n",
    "\n",
    "\n",
    "def predict_neuralprophet(model, train_df, holdout_df, store_id):\n",
    "    \"\"\"\n",
    "    Make predictions using trained NeuralProphet model.\n",
    "    \"\"\"\n",
    "    train_prophet = prepare_neuralprophet_data(train_df, store_id)\n",
    "\n",
    "    store_future = holdout_df[holdout_df['store_id'] == store_id].copy()\n",
    "    store_future = store_future.sort_values('date').reset_index(drop=True)\n",
    "    n_future = len(store_future)\n",
    "\n",
    "    future_rows = pd.DataFrame({\n",
    "        'ds': store_future['date'].values,\n",
    "        'y': [None] * n_future\n",
    "    })\n",
    "\n",
    "    for col in FUTURE_REGRESSORS:\n",
    "        if col in store_future.columns:\n",
    "            future_rows[col] = store_future[col].values\n",
    "        else:\n",
    "            future_rows[col] = 0\n",
    "\n",
    "    combined_df = pd.concat([train_prophet, future_rows], ignore_index=True)\n",
    "    forecast = model.predict(combined_df)\n",
    "\n",
    "    train_end_date = train_prophet['ds'].max()\n",
    "    future_forecast = forecast[forecast['ds'] > train_end_date].reset_index(drop=True)\n",
    "\n",
    "    if 'yhat1' in future_forecast.columns:\n",
    "        predictions = []\n",
    "        for i in range(min(n_future, len(future_forecast))):\n",
    "            col_name = f'yhat{i+1}'\n",
    "            if col_name in future_forecast.columns and i < len(future_forecast):\n",
    "                predictions.append(future_forecast.loc[i, col_name])\n",
    "            elif 'yhat1' in future_forecast.columns:\n",
    "                predictions.append(future_forecast.loc[i, 'yhat1'])\n",
    "        predictions = np.array(predictions, dtype=float)\n",
    "    else:\n",
    "        predictions = future_forecast['yhat'].values\n",
    "\n",
    "    predictions = np.expm1(predictions)\n",
    "\n",
    "    if len(predictions) < n_future:\n",
    "        last_val = predictions[-1] if len(predictions) > 0 else 0\n",
    "        predictions = np.concatenate([predictions, [last_val] * (n_future - len(predictions))])\n",
    "    predictions = predictions[:n_future]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "print(\"Model helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 - Prepare Submission Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast period: 2015-10-01 00:00:00 to 2015-12-31 00:00:00\n",
      "Forecast horizon: 92 days\n",
      "Total predictions needed: 1012\n"
     ]
    }
   ],
   "source": [
    "# Parse submission dates\n",
    "submission_dates = []\n",
    "for _, row in submission_df.iterrows():\n",
    "    store_id, date_str = parse_submission_id(row['id'])\n",
    "    date = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "    submission_dates.append({\n",
    "        'id': row['id'],\n",
    "        'store_id': store_id,\n",
    "        'date': date\n",
    "    })\n",
    "\n",
    "submission_forecast_df = pd.DataFrame(submission_dates)\n",
    "submission_forecast_df = prepare_calendar_features(submission_forecast_df, calendar_df)\n",
    "\n",
    "SUBMISSION_HORIZON = submission_forecast_df['date'].nunique()\n",
    "\n",
    "print(f\"Forecast period: {submission_forecast_df['date'].min()} to {submission_forecast_df['date'].max()}\")\n",
    "print(f\"Forecast horizon: {SUBMISSION_HORIZON} days\")\n",
    "print(f\"Total predictions needed: {len(submission_forecast_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 - Train Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NeuralProphet models on full dataset...\n",
      "Using: epochs=200, n_lags=84, lr=0.005\n",
      "============================================================\n",
      "\n",
      "Training Store 1: California – Sunset Plaza...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 2: California – Ocean View...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 3: California – Golden Hills...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 4: California – Redwood Center...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 5: Texas – Lone Star Mall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 6: Texas – Riverwalk Market...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 7: Texas – Alamo Heights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 8: Wisconsin – Maple Grove...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 9: Wisconsin – Lakeview Plaza...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.config.__post_init__) - Note: Fourier-based seasonality regularization is experimental.\n",
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.941% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done.\n",
      "\n",
      "Training Store 10: Wisconsin – Badger Crossing...\n",
      "  Done.\n",
      "\n",
      "============================================================\n",
      "All 10 models trained successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training NeuralProphet models on full dataset...\")\n",
    "print(f\"Using: epochs={EPOCHS}, n_lags={N_LAGS}, lr={LEARNING_RATE}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_models = {}\n",
    "\n",
    "for store_id in range(1, 11):\n",
    "    store_name = train_df[train_df['store_id'] == store_id]['store_name'].iloc[0]\n",
    "    print(f\"\\nTraining Store {store_id}: {store_name}...\")\n",
    "    \n",
    "    model, _ = train_neuralprophet_model(\n",
    "        full_train_with_events, \n",
    "        store_id,\n",
    "        forecast_horizon=SUBMISSION_HORIZON,\n",
    "        add_regressors=True,\n",
    "        epochs=EPOCHS,\n",
    "        n_lags=N_LAGS,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    final_models[store_id] = model\n",
    "    print(f\"  Done.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"All {len(final_models)} models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 - Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for stores 1-10...\n",
      "============================================================\n",
      "\n",
      "Predicting Store 1: California – Sunset Plaza...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 33,376.05, Min: 16,506.73, Max: 45,685.32\n",
      "\n",
      "Predicting Store 2: California – Ocean View...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 33,917.82, Min: 19,117.14, Max: 51,062.69\n",
      "\n",
      "Predicting Store 3: California – Golden Hills...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 49,346.16, Min: 23,943.32, Max: 65,288.28\n",
      "\n",
      "Predicting Store 4: California – Redwood Center...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 17,828.94, Min: 8,904.21, Max: 22,774.15\n",
      "\n",
      "Predicting Store 5: Texas – Lone Star Mall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 21,998.63, Min: 12,385.53, Max: 30,801.26\n",
      "\n",
      "Predicting Store 6: Texas – Riverwalk Market...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 27,588.54, Min: 14,574.08, Max: 37,660.30\n",
      "\n",
      "Predicting Store 7: Texas – Alamo Heights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 28,022.07, Min: 15,863.77, Max: 37,485.01\n",
      "\n",
      "Predicting Store 8: Wisconsin – Maple Grove...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 22,277.91, Min: 12,444.14, Max: 33,212.55\n",
      "\n",
      "Predicting Store 9: Wisconsin – Lakeview Plaza...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency D corresponds to 99.944% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - D\n",
      "INFO - (NP.data.processing._handle_missing_data) - Dropped 92 rows at the end with NaNs in 'y' column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 25,954.56, Min: 11,965.93, Max: 32,259.15\n",
      "\n",
      "Predicting Store 10: Wisconsin – Badger Crossing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 92 predictions\n",
      "  Mean: 20,659.43, Min: 10,796.20, Max: 29,391.28\n",
      "\n",
      "============================================================\n",
      "Generated 920 predictions for stores 1-10\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating predictions for stores 1-10...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_predictions = []\n",
    "\n",
    "for store_id in range(1, 11):\n",
    "    store_name = train_df[train_df['store_id'] == store_id]['store_name'].iloc[0]\n",
    "    print(f\"\\nPredicting Store {store_id}: {store_name}...\")\n",
    "    \n",
    "    model = final_models[store_id]\n",
    "    preds = predict_neuralprophet(model, full_train_with_events, submission_forecast_df, store_id)\n",
    "    \n",
    "    store_submission = submission_forecast_df[submission_forecast_df['store_id'] == store_id].copy()\n",
    "    store_submission = store_submission.sort_values('date').reset_index(drop=True)\n",
    "    store_submission['prediction'] = preds\n",
    "    final_predictions.append(store_submission[['id', 'prediction']])\n",
    "    \n",
    "    print(f\"  Generated {len(preds)} predictions\")\n",
    "    print(f\"  Mean: {preds.mean():,.2f}, Min: {preds.min():,.2f}, Max: {preds.max():,.2f}\")\n",
    "\n",
    "final_submission = pd.concat(final_predictions, ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Generated {len(final_submission)} predictions for stores 1-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08 - Store 0 Aggregation (Bottom-Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Store 0 predictions by summing stores 1-10...\n",
      "\n",
      "Store 0 predictions:\n",
      "  Mean: 280,970.11\n",
      "  Min: 150,085.80\n",
      "  Max: 361,117.21\n",
      "\n",
      "Total submission size: 1012\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Store 0 predictions by summing stores 1-10...\")\n",
    "\n",
    "store_1_10_preds = submission_forecast_df[submission_forecast_df['store_id'] != 0].copy()\n",
    "store_1_10_preds = store_1_10_preds.merge(\n",
    "    final_submission[['id', 'prediction']],\n",
    "    on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "store_0_preds = store_1_10_preds.groupby('date')['prediction'].sum().reset_index()\n",
    "store_0_preds['store_id'] = 0\n",
    "store_0_preds['id'] = store_0_preds['date'].apply(\n",
    "    lambda d: f\"0_{d.strftime('%Y%m%d')}\"\n",
    ")\n",
    "\n",
    "final_submission = pd.concat([\n",
    "    store_0_preds[['id', 'prediction']],\n",
    "    final_submission,\n",
    "], ignore_index=True)\n",
    "\n",
    "final_submission['prediction'] = final_submission['prediction'].clip(lower=0)\n",
    "\n",
    "print(f\"\\nStore 0 predictions:\")\n",
    "print(f\"  Mean: {store_0_preds['prediction'].mean():,.2f}\")\n",
    "print(f\"  Min: {store_0_preds['prediction'].min():,.2f}\")\n",
    "print(f\"  Max: {store_0_preds['prediction'].max():,.2f}\")\n",
    "print(f\"\\nTotal submission size: {len(final_submission)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 - Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUBMISSION SAVED\n",
      "============================================================\n",
      "\n",
      "File: ../submissions/neuralprophet/bottom_up_neuralprophet_20260103_224013.csv\n",
      "\n",
      "Summary:\n",
      "  Total predictions: 1012\n",
      "  Mean prediction: 51,085.47\n",
      "  Min prediction: 8,904.21\n",
      "  Max prediction: 361,117.21\n",
      "\n",
      "First 5 rows:\n",
      "           id     prediction\n",
      "0  0_20151001  258686.431560\n",
      "1  0_20151002  295067.523183\n",
      "2  0_20151003  352825.673482\n",
      "3  0_20151004  358673.183478\n",
      "4  0_20151005  290387.832959\n",
      "\n",
      "Last 5 rows:\n",
      "               id    prediction\n",
      "1007  10_20151227  21643.336867\n",
      "1008  10_20151228  17923.869029\n",
      "1009  10_20151229  15671.774324\n",
      "1010  10_20151230  14870.778711\n",
      "1011  10_20151231  16386.375785\n"
     ]
    }
   ],
   "source": [
    "submissions_dir = Path('../submissions/neuralprophet')\n",
    "submissions_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_path = submissions_dir / f'bottom_up_neuralprophet_{timestamp}.csv'\n",
    "\n",
    "final_submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"SUBMISSION SAVED\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"\\nFile: {output_path}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total predictions: {len(final_submission)}\")\n",
    "print(f\"  Mean prediction: {final_submission['prediction'].mean():,.2f}\")\n",
    "print(f\"  Min prediction: {final_submission['prediction'].min():,.2f}\")\n",
    "print(f\"  Max prediction: {final_submission['prediction'].max():,.2f}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(final_submission.head())\n",
    "print(f\"\\nLast 5 rows:\")\n",
    "print(final_submission.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time-series-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
