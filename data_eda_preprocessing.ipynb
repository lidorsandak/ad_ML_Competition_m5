{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9QuamwTHyLg8W+g91eEhe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lidorsandak/ad_ML_Competition_m5/blob/main/data_eda_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M5 Forecasting Challenge: Predicting Daily Walmart Revenue: EDA and Preprocessing\n",
        "\n",
        "**Course:** Advanced Machine Learning - Final Assignment\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "y8sqaZuZgc22"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-BVmUeefeNQ",
        "outputId": "965e3830-0e97-44f5-d8b5-ccecea19b630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup and Data Loading\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, you might need to install Optuna\n",
        "!pip install optuna -q\n",
        "\n",
        "import optuna\n",
        "\n",
        "# General imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, time, warnings, pickle, psutil, random\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "nkv31A6NgbL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Main Data"
      ],
      "metadata": {
        "id": "qCnNve43hWOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to your data on Google Drive\n",
        "DATA_PATH = '/content/drive/MyDrive/M5_Project/'\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
        "calendar_df = pd.read_csv(DATA_PATH + 'calendar_events.csv')\n",
        "sample_submission = pd.read_csv(DATA_PATH + 'forecast_submission.csv')"
      ],
      "metadata": {
        "id": "eDzmisCGhfDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore train data"
      ],
      "metadata": {
        "id": "8aphTZ7M1tfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('  unique forecasts: %i' % train_df.shape[0])\n",
        "for col in train_df.columns:\n",
        "  print('   N_unique %s: %i' % (col, train_df[col].nunique()))\n",
        "\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "0RUuSqxQkB6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing dates per store\n",
        "def check_missing_dates(df):\n",
        "    \"\"\"Identify gaps in the time series for each store\"\"\"\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    for store_id in df['store_id'].unique():\n",
        "        store_data = df[df['store_id'] == store_id].sort_values('date')\n",
        "        date_range = pd.date_range(start=store_data['date'].min(),\n",
        "                                     end=store_data['date'].max(),\n",
        "                                     freq='D')\n",
        "        missing_dates = date_range.difference(store_data['date'])\n",
        "\n",
        "        if len(missing_dates) > 0:\n",
        "            print(f\"Store {store_id}: {len(missing_dates)} missing dates\")\n",
        "            print(f\"  First few: {missing_dates[:5].tolist()}\")\n",
        "\n",
        "check_missing_dates(train_df)"
      ],
      "metadata": {
        "id": "wZ08R6po1wkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summary_by_store = train_df.groupby('store_id').agg(\n",
        "    min_date=('date', 'min'),\n",
        "    max_date=('date', 'max'),\n",
        "    min_revenue=('revenue', 'min'),\n",
        "    max_revenue=('revenue', 'max'),\n",
        "    avg_revenue=('revenue', 'mean') # It's also useful to see the average\n",
        ").reset_index()\n",
        "\n",
        "# Improve formatting for readability\n",
        "summary_by_store['min_revenue'] = summary_by_store['min_revenue'].round(2)\n",
        "summary_by_store['max_revenue'] = summary_by_store['max_revenue'].round(2)\n",
        "summary_by_store['avg_revenue'] = summary_by_store['avg_revenue'].round(2)\n",
        "\n",
        "# Display the resulting table\n",
        "print(\"Summary of Data Range and Revenue by Store ID:\")\n",
        "display(summary_by_store)"
      ],
      "metadata": {
        "id": "7IlAaIr7kJbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for zero/negative revenues (potential data issues)\n",
        "print(\"\\nZero revenue counts by store:\")\n",
        "print(train_df[train_df['revenue'] == 0].groupby('store_id').size())\n",
        "\n",
        "print(\"\\nNegative revenue counts:\")\n",
        "print((train_df['revenue'] < 0).sum())"
      ],
      "metadata": {
        "id": "z9jfyInn6910"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore submission dates"
      ],
      "metadata": {
        "id": "-xvFyMfp13JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The submission file ID is store_id + date (YYYYMMDD)\n",
        "# We need to parse the date part of the 'id' string.\n",
        "submission_ids = sample_submission['id'].str.split('_', expand=True)\n",
        "submission_ids.columns = ['store_id_str', 'date_str']\n",
        "submission_dates = pd.to_datetime(submission_ids['date_str'], format='%Y%m%d')\n",
        "\n",
        "H = submission_dates.nunique()\n",
        "print(f\"The Forecast Horizon (H) is: {H} days.\")\n",
        "\n",
        "# Now find the min and max dates\n",
        "min_submission_date = submission_dates.min()\n",
        "max_submission_date = submission_dates.max()\n",
        "\n",
        "# Calculate the duration (our forecast horizon H)\n",
        "num_forecast_days = (max_submission_date - min_submission_date).days + 1\n",
        "\n",
        "print(f\"Submission File Analysis:\")\n",
        "print(f\"-------------------------\")\n",
        "print(f\"Forecast Start Date: {min_submission_date.date()}\")\n",
        "print(f\"Forecast End Date:   {max_submission_date.date()}\")\n",
        "print(f\"Total Forecast Horizon (H): {num_forecast_days} days\")"
      ],
      "metadata": {
        "id": "Sgoz8yIxj5i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Data Merging and Preparation**"
      ],
      "metadata": {
        "id": "Qagxo40nhE7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date' columns to datetime objects\n",
        "train_df['date'] = pd.to_datetime(train_df['date'])\n",
        "calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
        "\n",
        "# Merge train_df with calendar_df to get all features in one place\n",
        "original_df = pd.merge(train_df, calendar_df, on='date', how='left')\n",
        "original_df['event'].fillna('NoEvent', inplace=True)\n",
        "\n",
        "print(\"\\nData merged:\")\n",
        "original_df.head()\n"
      ],
      "metadata": {
        "id": "nJL2saVTg_P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "\n",
        "\n",
        "## EDA\n",
        "\n",
        "\n",
        "\n",
        "*   Temporal Patterns Analysis\n",
        "    *   Overall Trend\n",
        "    *   Seasonality Detection\n",
        "    *   Year-over-Year Growth\n",
        "*   Store-Level Analysis\n",
        "    * Revenue Distribution Across Stores\n",
        "    * Store Time Series Comparison\n",
        "* Stationarity & Autocorrelation\n",
        "* Event/Anomaly Detection\n",
        "* Correlation Analysis\n",
        "* Decomposition Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "yQccSWSkicP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Temporal Patterns Analysis ---------------------\n",
        "\n",
        "# A. Overall Trend\n",
        "\n",
        "# Plot aggregate revenue over time\n",
        "df_agg = train_df[train_df['store_id'] == 0].copy()\n",
        "df_agg = df_agg.sort_values('date')\n",
        "\n",
        "# Plot 1: Aggregate Revenue (across the 10 real stores)\n",
        "train_stores= train_df[train_df['store_id'] != 0].copy()\n",
        "df_sum=train_stores.groupby('date')['revenue'].sum()\n",
        "df_sum.plot(title='Total Revenue Across 10 Stores', linewidth=0.8, figsize=(15, 5))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Weekly Seasonality\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(df_agg['date'], df_agg['revenue'], linewidth=0.8)\n",
        "plt.title('Total Revenue Over Time (All Stores)', fontsize=14)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Revenue')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# B. Seasonality Detection\n",
        "\n",
        "# Extract temporal features\n",
        "train_df['year'] = train_df['date'].dt.year\n",
        "train_df['month'] = train_df['date'].dt.month\n",
        "train_df['day_of_week'] = train_df['date'].dt.dayofweek\n",
        "train_df['quarter'] = train_df['date'].dt.quarter\n",
        "\n",
        "# Monthly seasonality\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Boxplot by month\n",
        "train_df[train_df['store_id'] == 0].boxplot(column='revenue', by='month', ax=axes[0])\n",
        "axes[0].set_title('Revenue Distribution by Month')\n",
        "axes[0].set_xlabel('Month')\n",
        "axes[0].set_ylabel('Revenue')\n",
        "\n",
        "# Average revenue by day of week\n",
        "train_df[train_df['store_id'] == 0].groupby('day_of_week')['revenue'].mean().plot(kind='bar', ax=axes[1])\n",
        "axes[1].set_title('Average Revenue by Day of Week')\n",
        "axes[1].set_xlabel('Day of Week (0=Mon, 6=Sun)')\n",
        "axes[1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=0)\n",
        "axes[1].set_ylabel('Average Revenue')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# C. Compare Same Time Periods\n",
        "\n",
        "# Filter to only Jan-Sep for all years to make fair comparisons\n",
        "df_total = train_df[train_df['store_id'] == 0].copy()\n",
        "df_total['date'] = pd.to_datetime(df_total['date'])\n",
        "df_total['year'] = df_total['date'].dt.year\n",
        "df_total['month'] = df_total['date'].dt.month\n",
        "\n",
        "# Only use Jan-Sep (months 1-9) for all years\n",
        "df_comparable = df_total[df_total['month'] <= 9]\n",
        "\n",
        "yearly_revenue = df_comparable.groupby('year')['revenue'].sum()\n",
        "\n",
        "print(\"Year-over-Year Revenue (Jan-Sep only):\")\n",
        "print(yearly_revenue)\n",
        "print(\"\\nYoY Growth Rate:\")\n",
        "yoy_growth = yearly_revenue.pct_change() * 100\n",
        "print(yoy_growth)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "yearly_revenue.plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Revenue by Year (Jan-Sep Only)', fontsize=12)\n",
        "axes[0].set_xlabel('Year')\n",
        "axes[0].set_ylabel('Revenue')\n",
        "axes[0].set_xticklabels(yearly_revenue.index, rotation=0)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Growth rate\n",
        "yoy_growth.dropna().plot(kind='bar', ax=axes[1], color=['red' if x < 0 else 'green' for x in yoy_growth.dropna()])\n",
        "axes[1].set_title('YoY Growth Rate (Jan-Sep)', fontsize=12)\n",
        "axes[1].set_xlabel('Year')\n",
        "axes[1].set_ylabel('Growth Rate (%)')\n",
        "axes[1].axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
        "axes[1].set_xticklabels(yoy_growth.dropna().index, rotation=0)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EHVw2igX7Xeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights and Modeling Implication**\n",
        "\n",
        "- Weekly Seasonality:\n",
        "    - Include day_of_week and is_weekend as features\n",
        "    - Consider a weekly seasonal component in time series models (e.g., seasonal_period=7 in SARIMA)\n",
        "- Monthly Seasonality:\n",
        "    - The business is not highly seasonal at the monthly level\n",
        "    - Events/holidays matter more than the month itself: Q4 outliers suggest end-of-year shopping boost\n",
        "    - For model we should handle the events outliers so they won't skew our model\n",
        "- Variance and Outliers\n",
        "    - heteroscedasticity - variance increases over time\n",
        "\n",
        "Holiday drops must be handled:\n",
        "- Flag them as outliers and exclude from training\n",
        "- Create is_holiday feature with special handling\n",
        "- Use a model robust to outliers (e.g., Huber loss)\n",
        "\n",
        "\n",
        "Key Takeaways:\n",
        "\n",
        "* Log transformation is critical here (np.log1p(revenue)) in order to stabilize the increasing variance and makes the model multiplicative rather than an additive model\n",
        "* Consider exogenous holiday variables\n",
        "* The trend is not perfectly linear - might benefit from polynomial features or tree-based models"
      ],
      "metadata": {
        "id": "4iYcXxsSBWdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Priority | Feature | Why |\n",
        "|---|---|---|\n",
        "| ðŸ”¥ **CRITICAL** | `day_of_week` / `is_weekend` | 40% weekend lift - massive signal |\n",
        "| ðŸ”¥ **CRITICAL** | `is_holiday` / `event` | Explains the near-zero revenue days |\n",
        "| ðŸ”¥ **CRITICAL** | Log transformation | Handles increasing variance |\n",
        "| â­ **Important** | Lag features (you have these) | Revenue is autocorrelated (weekly pattern) |\n",
        "| â­ **Important** | Rolling averages (you have these) | Smooth out noise, capture trends |\n",
        "| ðŸ’¡ **Helpful** | `month` / `quarter` | Mild seasonal effect, Q4 boost |\n",
        "| ðŸ’¡ **Helpful** | Year-based trend features | Capture the non-linear growth |"
      ],
      "metadata": {
        "id": "y7ZMLn8tGObp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Selection Guidance:\n",
        "Given these patterns, good model choices are:\n",
        "\n",
        "1. Tree-Based Models (XGBoost, LightGBM):\n",
        "- âœ… Handle non-linear growth pattern\n",
        "- âœ… Automatically learn interactions (weekend Ã— holiday)\n",
        "- âœ… Robust to outliers\n",
        "- âœ… Can handle your lag/rolling features well\n",
        "\n",
        "2. SARIMA / Prophet (if you want pure time series):\n",
        "- âœ… SARIMA with seasonal_order=(0,0,0,7) for weekly pattern\n",
        "- âœ… Prophet excellent for handling holidays/events\n",
        "- âš ï¸ May struggle with the non-linear growth trend\n",
        "\n",
        "3. Hybrid Approach (Recommended):\n",
        "- Use Prophet/SARIMA for trend + seasonality\n",
        "- Use XGBoost for the residuals (event effects, store-specific patterns)"
      ],
      "metadata": {
        "id": "VP37Y0EHGUoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Store-Level Analysis ---------------------\n",
        "\n",
        "# A.  Revenue Distribution Across Stores\n",
        "df_stores = train_df[train_df['store_id'] != 0].copy()\n",
        "\n",
        "# Average revenue by store\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "store_avg = df_stores.groupby('store_id')['revenue'].mean().sort_values()\n",
        "store_avg.plot(kind='barh', ax=axes[0])\n",
        "axes[0].set_title('Average Revenue by Store')\n",
        "axes[0].set_xlabel('Average Revenue')\n",
        "\n",
        "# Revenue variability by store (coefficient of variation)\n",
        "store_cv = (df_stores.groupby('store_id')['revenue'].std() /\n",
        "            df_stores.groupby('store_id')['revenue'].mean()).sort_values()\n",
        "store_cv.plot(kind='barh', ax=axes[1], color='coral')\n",
        "axes[1].set_title('Revenue Variability by Store (Coefficient of Variation)')\n",
        "axes[1].set_xlabel('CV (lower = more stable)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# B. Store Time Series Comparison\n",
        "# Plot individual store trends\n",
        "fig, axes = plt.subplots(5, 2, figsize=(15, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, store_id in enumerate(range(1, 11)):\n",
        "    store_data = train_df[train_df['store_id'] == store_id].sort_values('date')\n",
        "    axes[i].plot(store_data['date'], store_data['revenue'], linewidth=0.6)\n",
        "    axes[i].set_title(f'Store {store_id}', fontsize=10)\n",
        "    axes[i].tick_params(labelsize=8)\n",
        "    axes[i].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y_JXTIbZ_xX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Revenue Scale*\n",
        "\n",
        "**High Revenue Tier:**\n",
        "\n",
        "Store 3: ~$41K avg\n",
        "\n",
        "Store 1: ~$29K avg\n",
        "\n",
        "Store 6: ~$26K avg\n",
        "\n",
        "**Middle Tier:**\n",
        "\n",
        "Stores 7, 9, 10, 2, 5: ~$20-22K avg\n",
        "\n",
        "**Low Revenue Tier**\n",
        "\n",
        "Store 8: ~$18K avg\n",
        "\n",
        "Store 4: ~$15K avg\n",
        "\n",
        "\n",
        "Modeling Implication:\n",
        "- Separate models per store (recommended, given clear tiers)\n",
        "- Store ID as a categorical feature with embeddings (if using neural networks)\n",
        "- Hierarchical/grouped forecasting (forecast total, then allocate to stores)\n",
        "\n",
        "\n",
        "*Revenue Volatility: Stability vs. Unpredictability*\n",
        "\n",
        "Most Stable (Easiest to Forecast):\n",
        "\n",
        "- Store 6: CV â‰ˆ 0.21 (ðŸŽ¯ most predictable)\n",
        "- Store 4: CV â‰ˆ 0.23\n",
        "- Store 3: CV â‰ˆ 0.24\n",
        "\n",
        "Most Volatile (Hardest to Forecast):\n",
        "\n",
        "- Store 9: CV â‰ˆ 0.41 (âš ï¸ highly erratic)\n",
        "- Store 8: CV â‰ˆ 0.40\n",
        "- Store 2: CV â‰ˆ 0.28\n",
        "\n",
        "Store 3 (highest revenue) is relatively stable, while Store 4 (lowest revenue) is also stable. Meanwhile, mid-tier stores like 8 and 9 are highly volatile.\n",
        "\n",
        "Modeling Implication:\n",
        "- Separate error bounds for each store, Store 9 will need wider prediction intervals\n",
        "- More complex features needed for volatile stores (lag features, external data like weather/events)\n",
        "- Simpler models may work for stable stores (Store 3, 4, 6)\n",
        "Consider quantile regression for stores with high CV to capture uncertainty\n",
        "\n",
        "\n",
        "Key Takeaways:\n",
        "\n",
        "- Store 8\n",
        "\n",
        "\"# Create a regime indicator for Store 8\n",
        "df.loc[(df['store_id'] == 8) & (df['date'] < '2012-06-01'), 'store_8_regime'] = 0\n",
        "df.loc[(df['store_id'] == 8) & (df['date'] >= '2012-06-01'), 'store_8_regime'] = 1\"\n",
        "\n",
        "- Consider Store Clustering\n",
        "  \n",
        "  Instead of 10 separate models, group similar stores:\n",
        "  \n",
        "  Cluster A (High Growth): Stores 1, 3, 7\n",
        "\n",
        "      Use models with strong trend components\n",
        "\n",
        "  Cluster B (Stable/Mature): Stores 4, 6, 10\n",
        "\n",
        "      Emphasize seasonality over trend\n",
        "\n",
        "  Cluster C (Volatile): Stores 2, 8, 9\n",
        "\n",
        "      Use robust models, more features, wider prediction intervals\n",
        "\n",
        "  Cluster D (Middle): Store 5\n",
        "\n",
        "      Standard approach"
      ],
      "metadata": {
        "id": "ZkWqW_9BVw-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Stationarity & Autocorrelation ---------------------\n",
        "\n",
        "\n",
        "# Test for stationarity (Augmented Dickey-Fuller test)\n",
        "def test_stationarity(timeseries, title):\n",
        "    result = adfuller(timeseries.dropna())\n",
        "    print(f'\\n{title}')\n",
        "    print(f'ADF Statistic: {result[0]:.4f}')\n",
        "    print(f'p-value: {result[1]:.4f}')\n",
        "    print(f'Critical Values:')\n",
        "    for key, value in result[4].items():\n",
        "        print(f'  {key}: {value:.3f}')\n",
        "\n",
        "    if result[1] <= 0.05:\n",
        "        print(\"âœ“ Series is stationary (reject null hypothesis)\")\n",
        "    else:\n",
        "        print(\"âœ— Series is non-stationary (fail to reject null hypothesis)\")\n",
        "\n",
        "# Test on aggregate revenue\n",
        "df_total = train_df[train_df['store_id'] == 0].sort_values('date').set_index('date')\n",
        "test_stationarity(df_total['revenue'], 'Total Revenue Stationarity Test')\n",
        "\n",
        "# ACF/PACF plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
        "plot_acf(df_total['revenue'].dropna(), lags=50, ax=axes[0])\n",
        "plot_pacf(df_total['revenue'].dropna(), lags=50, ax=axes[1])\n",
        "axes[0].set_title('Autocorrelation Function (ACF)')\n",
        "axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hE2fMK_u_xxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Event/Anomaly Detection ---------------------\n",
        "# Detect outliers using IQR method\n",
        "def detect_outliers(train_df, store_id):\n",
        "    store_data = train_df[train_df['store_id'] == store_id].copy()\n",
        "    Q1 = store_data['revenue'].quantile(0.25)\n",
        "    Q3 = store_data['revenue'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    outliers = store_data[\n",
        "        (store_data['revenue'] < Q1 - 1.5 * IQR) |\n",
        "        (store_data['revenue'] > Q3 + 1.5 * IQR)\n",
        "    ]\n",
        "\n",
        "    return outliers[['date', 'revenue']].sort_values('date')\n",
        "\n",
        "# Check outliers for total revenue\n",
        "print(\"Potential outlier dates (All Stores):\")\n",
        "outliers = detect_outliers(train_df, 0)\n",
        "print(outliers.head(10))\n",
        "\n",
        "# Visualize outliers\n",
        "df_total = train_df[train_df['store_id'] == 0].sort_values('date')\n",
        "Q1 = df_total['revenue'].quantile(0.25)\n",
        "Q3 = df_total['revenue'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(df_total['date'], df_total['revenue'], linewidth=0.8, label='Revenue')\n",
        "plt.axhline(Q3 + 1.5*IQR, color='r', linestyle='--', label='Upper Outlier Threshold')\n",
        "plt.axhline(Q1 - 1.5*IQR, color='r', linestyle='--', label='Lower Outlier Threshold')\n",
        "plt.title('Revenue with Outlier Thresholds')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6eDgUWnb_-IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Consider corrected Outlier Detection Strategy\n",
        "\n",
        "Event/Anomaly Insights\n",
        "âœ… Confirmed Patterns:\n",
        "\n",
        "- Christmas Day = near-zero revenue every year (100% predictable)\n",
        "- Multiple major holidays cause 99% revenue drops (need explicit handling)\n",
        "- 2015 has unusual high-revenue days (requires investigation)\n",
        "- IQR method flags 2015 growth as outliers (need year-specific thresholds)\n",
        "\n",
        "\n",
        "ðŸš¨ Critical Actions:\n",
        "\n",
        "- Add holiday features (is_christmas, is_major_holiday)\n",
        "- Investigate 2015 high outliers - check event column and day_of_week\n",
        "- Don't remove outliers blindly - holidays are predictable, not anomalies\n",
        "- Consider separate models for holiday vs. non-holiday days\n",
        "\n",
        "\"# Option 1: Explicit features (best for tree models)\n",
        "df['is_christmas'] = (df['date'].dt.month == 12) & (df['date'].dt.day == 25)\n",
        "df['is_christmas'] = df['is_christmas'].astype(int)\n",
        "\n",
        "\"# Option 2: Exclude from training (for some models)\n",
        "df_train = df[df['revenue'] > 1000]  # Remove near-zero days\n",
        "\n",
        "\"# Option 3: Use a library like 'holidays'\n",
        "import holidays\n",
        "us_holidays = holidays.US()\n",
        "df['is_holiday'] = df['date'].apply(lambda x: x in us_holidays).astype(int)"
      ],
      "metadata": {
        "id": "rB04VLAiECzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ------------------- Correlation Analysis ---------------------\n",
        "\n",
        "# If you have the 'event' column\n",
        "if 'event' in train_df.columns:\n",
        "    # Revenue by event type\n",
        "    event_revenue = train_df[train_df['store_id'] == 0].groupby('event')['revenue'].agg(['mean', 'median', 'count'])\n",
        "    print(\"\\nRevenue by Event Type:\")\n",
        "    print(event_revenue.sort_values('mean', ascending=False))\n",
        "\n",
        "# Weekend vs Weekday\n",
        "train_df['is_weekend'] = train_df['day_of_week'].isin([5, 6]).astype(int)\n",
        "weekend_comparison = train_df[train_df['store_id'] == 0].groupby('is_weekend')['revenue'].mean()\n",
        "print(f\"\\nWeekday avg: ${weekend_comparison[0]:,.2f}\")\n",
        "print(f\"Weekend avg: ${weekend_comparison[1]:,.2f}\")\n",
        "print(f\"Weekend lift: {(weekend_comparison[1]/weekend_comparison[0] - 1)*100:.1f}%\")"
      ],
      "metadata": {
        "id": "ehaC9fZlAD5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- Decomposition Analysis ---------------------\n",
        "\n",
        "\n",
        "\n",
        "# Seasonal decomposition\n",
        "df_total = train_df[train_df['store_id'] == 0].sort_values('date').set_index('date')\n",
        "\n",
        "decomposition = seasonal_decompose(df_total['revenue'], model='additive', period=7)\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(15, 10))\n",
        "decomposition.observed.plot(ax=axes[0], title='Observed')\n",
        "decomposition.trend.plot(ax=axes[1], title='Trend')\n",
        "decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
        "decomposition.resid.plot(ax=axes[3], title='Residual')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1SsYq1WNAJT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize the revenue distribution\n",
        "sns.histplot(original_df['revenue'], kde=True, bins=50)\n",
        "plt.title('Distribution of Revenue (Original)')\n",
        "plt.show()\n",
        "\n",
        "# Now apply log1p and visualize again\n",
        "sns.histplot(np.log1p(original_df['revenue']), kde=True, bins=50)\n",
        "plt.title('Distribution of Revenue (Log-Transformed)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vmfGat5KF5HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Feature Engineering\n",
        "\n"
      ],
      "metadata": {
        "id": "2RlO6Q_htSyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features(data):\n",
        "    \"\"\"\n",
        "    Creates time series features from a datetime index.\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid modifying the original dataframe\n",
        "    new_df = data.copy()\n",
        "\n",
        "    # --- 1. Date-based Features ---\n",
        "    # These help the model learn seasonal patterns\n",
        "    new_df['date'] = pd.to_datetime(new_df['date'])\n",
        "    new_df['day_of_week'] = new_df['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "    new_df['day_of_month'] = new_df['date'].dt.day\n",
        "    new_df['week_of_year'] = new_df['date'].dt.isocalendar().week.astype(int)\n",
        "    new_df['month'] = new_df['date'].dt.month\n",
        "    new_df['year'] = new_df['date'].dt.year\n",
        "    new_df['is_weekend'] = (new_df['day_of_week'] >= 5).astype(int) # 1 for weekend, 0 for weekday\n",
        "\n",
        "    # --- 2. Lag Features ---\n",
        "    h_forecast = H\n",
        "    lags = [h_forecast, h_forecast + 7, h_forecast + 14, h_forecast + 28]\n",
        "\n",
        "    # Sort data to ensure lags are correct\n",
        "    new_df = new_df.sort_values(['store_id', 'date'])\n",
        "\n",
        "    for lag in lags:\n",
        "        new_df[f'revenue_lag_{lag}'] = new_df.groupby('store_id')['revenue'].shift(lag)\n",
        "\n",
        "    # --- 3. Rolling Window Features ---\n",
        "    # These capture recent trends and momentum.\n",
        "    # We use a window that starts after the lag period.\n",
        "    window_sizes = [7, 14, 28,46]\n",
        "    for window in window_sizes:\n",
        "        # The shift of h_forecast is crucial to prevent data leakage\n",
        "        rolling_series = new_df.groupby('store_id')['revenue'].shift(h_forecast).rolling(window)\n",
        "        new_df[f'revenue_roll_mean_{window}'] = rolling_series.mean()\n",
        "        new_df[f'revenue_roll_std_{window}'] = rolling_series.std()\n",
        "\n",
        "    # --- 4. Event Features ---\n",
        "    # Convert text-based events into a format the model can use.\n",
        "    # We will use one-hot encoding later, but for now we can make it a categorical type.\n",
        "    new_df['event'] = new_df['event'].astype('category')\n",
        "\n",
        "    # Convert store_id to category as well, so the model treats it as a label, not a number.\n",
        "    new_df['store_id'] = new_df['store_id'].astype('category')\n",
        "\n",
        "    return new_df"
      ],
      "metadata": {
        "id": "6sGC0UJttV4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Apply the feature engineering function ---\n",
        "#  Do we need log-transform to the revenue? original_df['revenue'] = np.log1p(original_df['revenue'])\n",
        "df_featured = create_features(original_df)\n",
        "df_featured['is_event'] = (df_featured['event'] != 'NoEvent').astype(int)\n",
        "# Drop rows with NaN values created by lags and rolling windows\n",
        "df_featured.dropna(inplace=True)\n",
        "print(df_featured.head())\n",
        "print(df_featured.columns)\n"
      ],
      "metadata": {
        "id": "buAyMRDGGn_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if these are specific days of the week\n",
        "df_store=df_featured[df_featured['store_id'] == 0]\n",
        "high_outliers = df_store[df_store['date'].isin(['2015-01-31', '2015-02-07', '2015-02-08',\n",
        "                                     '2015-03-08', '2015-03-15', '2015-04-12'])]\n",
        "print(high_outliers[['date', 'revenue','store_id' ,'day_of_week', 'event']])\n",
        "\n",
        "# Check if this is a data error - are they all the same day pattern?\n",
        "print(high_outliers['day_of_week'].value_counts())\n",
        "\n",
        "# Compare to normal 2015 Saturdays\n",
        "normal_saturdays_2015 = df_store[(df_store['year'] == 2015) & (df_store['day_of_week'] == 5) &\n",
        "                            (~df_store['date'].isin(high_outliers['date']))]\n",
        "print(f\"Normal 2015 Saturday avg: {normal_saturdays_2015['revenue'].mean()}\")"
      ],
      "metadata": {
        "id": "Pu-QzrNlr_CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save in drive"
      ],
      "metadata": {
        "id": "01utyLOm3dKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save Processed DataFrames ---\n",
        "\n",
        "PROJECT_PATH = '/content/drive/MyDrive/M5_Project/'\n",
        "output_folder = PROJECT_PATH + 'processed_data/'\n",
        "\n",
        "# 1. Save the main featured dataframe\n",
        "df_featured.to_parquet(output_folder + 'df_featured.parquet', engine='pyarrow')\n",
        "\n",
        "# 2. Save the original merged dataframe\n",
        "original_df.to_parquet(output_folder + 'original_df.parquet', engine='pyarrow')\n",
        "\n",
        "print(f\"Successfully saved files to: {output_folder}\")\n",
        "print(os.listdir(output_folder)) # List files to confirm they were saved"
      ],
      "metadata": {
        "id": "SpQXRyJA3hX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split train/test"
      ],
      "metadata": {
        "id": "YIGfm7cN6tfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- train and validation sets from the FEATURED data ---\n",
        "last_train_date_featured = df_featured['date'].max() - pd.to_timedelta(H, unit='D')\n",
        "validation_start_date_featured = last_train_date_featured + pd.to_timedelta(1, unit='D')\n",
        "\n",
        "train_final = df_featured[df_featured['date'] <= last_train_date_featured]\n",
        "val_final = df_featured[df_featured['date'] >= validation_start_date_featured]\n",
        "\n",
        "# --- Define our features (X) and target (y) ---\n",
        "TARGET = 'revenue'\n",
        "\n",
        "FEATURES = [col for col in train_final.columns if col not in [TARGET, 'store_name', 'date', 'weekday', 'event']]\n",
        "\n",
        "X_train = train_final[FEATURES]\n",
        "y_train = train_final[TARGET]\n",
        "\n",
        "X_val = val_final[FEATURES]\n",
        "y_val = val_final[TARGET]\n",
        "\n",
        "print(\"Training data shape (X, y):\", X_train.shape, y_train.shape)\n",
        "print(\"Validation data shape (X, y):\", X_val.shape, y_val.shape)\n",
        "print(\"\\nHere are a few of the features we created:\")\n",
        "print(X_train.head())"
      ],
      "metadata": {
        "id": "VtivtdVDGoa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df.columns"
      ],
      "metadata": {
        "id": "TyQogzPhCjNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iu2wVJoemJJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}