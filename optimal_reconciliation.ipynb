{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6JwdvtWfIvVRwV3dchY4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lidorsandak/ad_ML_Competition_m5/blob/main/optimal_reconciliation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasetsforecast hierarchicalforecast neuralforecast statsforecast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_c9TM-FZgYu",
        "outputId": "5dab996a-2733-4903-8d19-3bd8f41cb698"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasetsforecast in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: hierarchicalforecast in /usr/local/lib/python3.12/dist-packages (1.3.1)\n",
            "Requirement already satisfied: neuralforecast in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: statsforecast in /usr/local/lib/python3.12/dist-packages (2.0.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasetsforecast) (3.13.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from datasetsforecast) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from datasetsforecast) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasetsforecast) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from datasetsforecast) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from datasetsforecast) (4.67.1)\n",
            "Requirement already satisfied: utilsforecast>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from datasetsforecast) (0.2.15)\n",
            "Requirement already satisfied: xlrd>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasetsforecast) (2.0.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from hierarchicalforecast) (0.60.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from hierarchicalforecast) (3.10.0)\n",
            "Requirement already satisfied: narwhals>=2.0 in /usr/local/lib/python3.12/dist-packages (from hierarchicalforecast) (2.13.0)\n",
            "Requirement already satisfied: qpsolvers[clarabel] in /usr/local/lib/python3.12/dist-packages (from hierarchicalforecast) (4.8.2)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.12/dist-packages (from hierarchicalforecast) (2025.3.1)\n",
            "Requirement already satisfied: coreforecast>=0.0.6 in /usr/local/lib/python3.12/dist-packages (from neuralforecast) (0.0.16)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from neuralforecast) (2025.3.0)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from neuralforecast) (2.9.0+cpu)\n",
            "Requirement already satisfied: pytorch-lightning>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from neuralforecast) (2.6.0)\n",
            "Requirement already satisfied: ray>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from ray[tune]>=2.2.0->neuralforecast) (2.53.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (from neuralforecast) (4.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from statsforecast) (3.1.2)\n",
            "Requirement already satisfied: scipy<1.16.0,>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from statsforecast) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from statsforecast) (0.14.6)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from statsforecast) (0.9.4)\n",
            "Requirement already satisfied: threadpoolctl>=3 in /usr/local/lib/python3.12/dist-packages (from statsforecast) (3.6.0)\n",
            "Requirement already satisfied: triad>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from fugue>=0.8.1->statsforecast) (1.0.0)\n",
            "Requirement already satisfied: adagio>=0.2.6 in /usr/local/lib/python3.12/dist-packages (from fugue>=0.8.1->statsforecast) (0.2.6)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->hierarchicalforecast) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasetsforecast) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasetsforecast) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasetsforecast) (2025.3)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast) (6.0.3)\n",
            "Requirement already satisfied: torchmetrics>0.7.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast) (4.15.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast) (0.15.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (8.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (3.20.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (5.29.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3 in /usr/local/lib/python3.12/dist-packages (from ray[tune]>=2.2.0->neuralforecast) (2.12.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.12/dist-packages (from ray[tune]>=2.2.0->neuralforecast) (2.6.4)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from ray[tune]>=2.2.0->neuralforecast) (18.1.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->datasetsforecast) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.13.2->statsforecast) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->neuralforecast) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->neuralforecast) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->neuralforecast) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->neuralforecast) (3.1.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasetsforecast) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasetsforecast) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasetsforecast) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasetsforecast) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasetsforecast) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasetsforecast) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasetsforecast) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->hierarchicalforecast) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->hierarchicalforecast) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->hierarchicalforecast) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->hierarchicalforecast) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->hierarchicalforecast) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->hierarchicalforecast) (3.2.5)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->neuralforecast) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna->neuralforecast) (6.10.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->neuralforecast) (2.0.45)\n",
            "Requirement already satisfied: clarabel>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from qpsolvers[clarabel]->hierarchicalforecast) (0.11.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->datasetsforecast) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->datasetsforecast) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->datasetsforecast) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->datasetsforecast) (2025.11.12)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->neuralforecast) (1.3.10)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.4.1->qpsolvers[clarabel]->hierarchicalforecast) (2.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[tune]>=2.2.0->neuralforecast) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[tune]>=2.2.0->neuralforecast) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[tune]>=2.2.0->neuralforecast) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasetsforecast) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->neuralforecast) (3.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->neuralforecast) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->neuralforecast) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (0.30.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.4.1->qpsolvers[clarabel]->hierarchicalforecast) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from datasetsforecast.hierarchical import HierarchicalData\n",
        "from hierarchicalforecast.utils import aggregate, HierarchicalPlot\n",
        "from neuralforecast.utils import augment_calendar_df\n",
        "from utilsforecast.plotting import plot_series"
      ],
      "metadata": {
        "id": "Tq1NNL0y7Tvv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#obtain hierarchical dataset\n",
        "from datasetsforecast.hierarchical import HierarchicalData\n",
        "\n",
        "# compute base forecast no coherent\n",
        "from statsforecast.core import StatsForecast\n",
        "from statsforecast.models import AutoARIMA, Naive\n",
        "\n",
        "#obtain hierarchical reconciliation methods and evaluation\n",
        "from hierarchicalforecast.core import HierarchicalReconciliation\n",
        "from hierarchicalforecast.evaluation import evaluate\n",
        "from hierarchicalforecast.methods import BottomUp, TopDown, MiddleOut\n",
        "from utilsforecast.losses import mse"
      ],
      "metadata": {
        "id": "1gPaNz2f71CF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Correct Imports for hierarchicalforecast v1.0.0 ---\n",
        "\n",
        "# from hierarchicalforecast.core import HierarchicalForecast\n",
        "# from hierarchicalforecast.utils import is_strictly_hierarchical\n",
        "# from hierarchicalforecast.reconciliation import MinT\n",
        "\n",
        "# import lightgbm as lgb\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# print(\"Successfully imported HierarchicalForecast and dependencies!\")"
      ],
      "metadata": {
        "id": "aidwb8Z-4fG2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TLyDpN3zMpe",
        "outputId": "999915e6-eb72-4d5b-cc71-563ae8632321"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M5 Forecasting Challenge: Predicting Daily Walmart Revenue\n",
        "\n",
        "**Course:** Advanced Machine Learning - Final Assignment\n",
        "\n",
        "---\n",
        "\n",
        "### Project Objective\n",
        "\n",
        "The goal of this project is to develop a high-performance forecasting model to predict the daily revenue for 10 individual Walmart stores and an aggregate of all stores. The project is inspired by the M5 Forecasting Competition and uses a modified version of its dataset.\n",
        "\n",
        "Success is measured by the Root Mean Squared Error (RMSE)on a hidden test set, with the primary goal of achieving a lower RMSE than the provided baseline of 11,761 and competing for the top position on the class leaderboard.\n",
        "\n",
        "### Methodology & Approach\n",
        "\n",
        "This notebook follows a structured, end-to-end machine learning pipeline to tackle the forecasting problem:\n",
        "\n",
        "1.  Exploratory Data Analysis (EDA)\n",
        "2.  Validation Strategy\n",
        "3.  Feature Engineering\n",
        "4.  Modeling\n",
        "5.  Hyperparameter Tuning\n",
        "6.  Final Forecasting & Submission\n",
        "---"
      ],
      "metadata": {
        "id": "he8fQN_Dqvh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Set Env**\n",
        "\n",
        "*   Drive mount\n",
        "*   Imports\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mq96PcynKMvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oTtjF2Q3myr",
        "outputId": "ce080afd-da7c-4bb9-ef61-51d57c999102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup and Data Loading\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define path to your data on Google Drive\n",
        "PROJECT_PATH = '/content/drive/MyDrive/M5_Project/'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#import optuna\n",
        "\n",
        "# General imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, time, warnings, pickle, psutil, random\n",
        "\n",
        "# from neuralforecast.utils import augment_calendar_df\n",
        "# from hierarchicalforecast.utils import is_strictly_hierarchical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "3L0s73dZ55wc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data**\n",
        "\n",
        "Case 1: use raw data\n",
        "1. Load Data\n",
        "2. Data Merging and Preparation\n",
        "3. Modify to fit model\n",
        "\n",
        "\n",
        "Case 2: Use preprocessed data from drive\n",
        "1. Load data\n",
        "2. Modify to f imodel\n"
      ],
      "metadata": {
        "id": "GkEViTef8b-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ---------------------------- case 1: ------------------------------------\n",
        "\n",
        "# # Load data\n",
        "# train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
        "# calendar_df = pd.read_csv(DATA_PATH + 'calendar_events.csv')\n",
        "# sample_submission = pd.read_csv(DATA_PATH + 'forecast_submission.csv')\n",
        "# print(\"Raw Data loaded successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------- case 2: ------------------------------------\n",
        "\n",
        "# Load data\n",
        "input_folder = PROJECT_PATH + 'processed_data/'\n",
        "# Load the dataframes from the Parquet files\n",
        "df_featured = pd.read_parquet(input_folder + 'df_featured.parquet', engine='pyarrow')\n",
        "original_df = pd.read_parquet(input_folder + 'original_df.parquet', engine='pyarrow')\n",
        "\n",
        "\n",
        "print(\"Processed Data loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nRZAGKqzbHg",
        "outputId": "b1a61c3b-77e8-454e-ff20-5c77ec101e56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Data loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare Data for HierarchicalForecast ---\n",
        "full_df = original_df.copy()\n",
        "Y_df = full_df[['date', 'store_id', 'revenue']].copy()\n",
        "Y_df.rename(columns={'date': 'ds', 'store_id': 'unique_id', 'revenue': 'y'}, inplace=True)\n",
        "Y_df['unique_id'] = Y_df['unique_id'].astype(str)\n",
        "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
        "\n",
        "#Y_df = pd.get_dummies(Y_df, columns=['event'], drop_first=True)\n",
        "\n",
        "\n",
        "display(Y_df.head())\n",
        "print(f\"Unique IDs being used: {sorted(Y_df['unique_id'].unique(), key=int)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KpUm_FpucmLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part B: Create S_df (The Summing Matrix) ---\n",
        "# The S matrix defines how the bottom-level series sum up to the top levels. The columns of S are the bottom-level series. The rows of S are ALL series (bottom + top).\n",
        "\n",
        "# Define levels\n",
        "bottom_level_series = [str(i) for i in range(1, 11)]\n",
        "top_level_series = ['0']\n",
        "all_series = top_level_series + bottom_level_series\n",
        "\n",
        "# Create the matrix as a pandas DataFrame\n",
        "S_df = pd.DataFrame(0, index=all_series, columns=bottom_level_series)\n",
        "\n",
        "# The row for the top level ('0') is the sum of all bottom levels. So, set it to 1.\n",
        "S_df.loc['0', :] = 1\n",
        "\n",
        "# The rows for the bottom levels are just themselves. This creates an identity matrix in the lower part.\n",
        "for store_id in bottom_level_series:\n",
        "    S_df.loc[store_id, store_id] = 1\n",
        "\n",
        "print(\"\\nS_df (Summing Matrix) constructed manually:\")\n",
        "display(S_df)"
      ],
      "metadata": {
        "id": "58cUp5OhGVFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission data preprocessing"
      ],
      "metadata": {
        "id": "zNCSbreKhcsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_ids = sample_submission['id'].str.split('_', expand=True)\n",
        "submission_ids.columns = ['store_id_str', 'date_str']\n",
        "submission_dates = pd.to_datetime(submission_ids['date_str'], format='%Y%m%d')\n",
        "# The forecast horizon H is the number of unique dates in the submission file\n",
        "H = submission_dates.nunique()\n",
        "print(f\"The Forecast Horizon (H) is: {H} days.\")"
      ],
      "metadata": {
        "id": "BuVRRFc58iZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exogenous Features"
      ],
      "metadata": {
        "id": "OQICwOWqqXpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Augment calendar features for the historical data.\n",
        "Y_df_augmented, calendar_cols = augment_calendar_df(df=Y_df, freq='D')\n",
        "\n",
        "future_dates = pd.date_range(Y_df['ds'].max() + pd.Timedelta(days=1), periods=H, freq='D')\n",
        "X_future_df = pd.DataFrame({'ds': future_dates})\n",
        "X_future_df['unique_id'] = 'placeholder'\n",
        "\n",
        "X_future_df, _ = augment_calendar_df(df=X_future_df, freq='D')\n",
        "\n",
        "# X_df is the dataframe of exogenous features for the historical period\n",
        "X_df = Y_df_augmented[calendar_cols].copy()\n",
        "\n",
        "Y_df = Y_df_augmented[['ds', 'unique_id', 'y']].copy()\n",
        "\n",
        "print(\"Created calendar features for past and future dates.\")\n",
        "print(\"\\nHistorical Features (X_df):\")\n",
        "display(X_df.head())\n",
        "print(\"\\nFuture Features (X_future_df):\")\n",
        "display(X_future_df.head())"
      ],
      "metadata": {
        "id": "KtGZldPdqdHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tags = {\n",
        "#     '0': [str(i) for i in range(1, 11)]\n",
        "# }\n",
        "# # Here we plot the hierarchical constraints matrix\n",
        "# hplot = HierarchicalPlot(S=S_df, tags=tags)\n",
        "# hplot.plot_summing_matrix()\n",
        "\n",
        "# # plot_series(forecasts_df=Y_df[[\"unique_id\", \"ds\", \"y\"]], ids=['TotalAll'])"
      ],
      "metadata": {
        "id": "GEf_Wx-N3rB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set Train / Validation data"
      ],
      "metadata": {
        "id": "_YDk3t6thjWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Using a forecast horizon (H) of: {H} days for the validation split.\")\n",
        "\n",
        "# Determine the cutoff date for the split\n",
        "split_date = Y_df['ds'].max() - pd.to_timedelta(H, unit='D')\n",
        "\n",
        "# Split all our dataframes based on this date\n",
        "Y_train = Y_df[Y_df['ds'] <= split_date]\n",
        "Y_val = Y_df[Y_df['ds'] > split_date]\n",
        "\n",
        "X_train = X_df.loc[Y_train.index]\n",
        "X_val = X_df.loc[Y_val.index]\n",
        "\n",
        "print(f\"Training data from {Y_train['ds'].min().date()} to {Y_train['ds'].max().date()}\")\n",
        "print(f\"Validation data from {Y_val['ds'].min().date()} to {Y_val['ds'].max().date()}\")\n",
        "print(f\"\\nY_train shape: {Y_train.shape}, Y_val shape: {Y_val.shape}\")\n",
        "print(f\"X_train shape: {X_train.shape}, X_val shape: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "fyYUhUyG5j7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Your Hierarchy Structure"
      ],
      "metadata": {
        "id": "40MPAgIjh17k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # The top level (store 0) is the sum of all other stores (1 through 10)\n",
        "# hierarchy_structure = {\n",
        "#     '0': [str(i) for i in range(1, 11)]\n",
        "# }\n",
        "\n",
        "# # In HierarchicalForecast, we define the Summing Matrix 'S' based on this.\n",
        "# # The library can automatically create this matrix for you from the tags.\n",
        "# # We just need to define the relationships. Let's create the tags:\n",
        "# tags = {}\n",
        "# for node in hierarchy_structure:\n",
        "#     for bottom_node in hierarchy_structure[node]:\n",
        "#         if bottom_node not in tags:\n",
        "#             tags[bottom_node] = []\n",
        "#         tags[bottom_node].append(node)\n",
        "\n",
        "# print(\"Hierarchy tags defined.\")\n",
        "# print(tags)"
      ],
      "metadata": {
        "id": "RdCRI-sWh3vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "h7okAAWhiWt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "# from statsforecast.models import SklearnRegressor\n",
        "from neuralforecast import NeuralForecast\n",
        "from hierarchicalforecast.methods import BottomUp, MinTrace"
      ],
      "metadata": {
        "id": "oBmexWae6G0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {\n",
        "    'learning_rate': 0.0443, 'num_leaves': 242, 'max_depth': 10,\n",
        "    # ... include all other params from your best trial ...\n",
        "    'objective': 'regression_l1', 'random_state': 42, 'n_jobs':-1,\n",
        "}"
      ],
      "metadata": {
        "id": "FErsShUH8ZDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- 1. Let's get your best LGBM parameters from Optuna ---\n",
        "# (Make sure best_trial.params is available from your tuning step)\n",
        "best_params = best_trial.params.copy()\n",
        "# Remove keys that are for Optuna's suggestion process, not for LGBM itself\n",
        "best_params.pop('n_estimators', None)\n",
        "\n",
        "# --- 2. Instantiate the HierarchicalForecast object ---\n",
        "# We will tell it to use your LGBM model for each series.\n",
        "# The `lags` and `lag_transforms` arguments are how it internally creates features!\n",
        "# This is a powerful, built-in feature engineering step.\n",
        "models_to_run = [\n",
        "    lgb.LGBMRegressor(**best_params)\n",
        "]\n",
        "\n",
        "hf = HierarchicalForecast(\n",
        "    models=models_to_run,\n",
        "    freq='D', # Daily frequency\n",
        "    reconciler=MinT(method='wls_struct'), # The MinT reconciler!\n",
        "    lag_features_kwargs={'lags': [7,14,28], 'agg_fns': ['mean', 'std']}, # Simplified features for this example\n",
        "    num_threads=4\n",
        ")\n",
        "\n",
        "# --- 3. Generate the forecasts ---\n",
        "# The forecast method will automatically:\n",
        "# 1. Split data into train/test\n",
        "# 2. Train a model for each series\n",
        "# 3. Predict for the horizon H\n",
        "# 4. Reconcile the predictions\n",
        "# The library requires Y_df (the dataframe) and S (the summing matrix), which it can build from `tags`.\n",
        "# Note: For a real prediction, you fit on ALL data and predict on future dates.\n",
        "# The following is a conceptual example. Let's adapt it for YOUR specific recursive challenge.\n",
        "# This library is best for when you don't need a complex recursive strategy.\n",
        "# Since we have one, let's adapt it slightly.\n",
        "\n",
        "# --- ADAPTED STRATEGY FOR YOUR RECURSIVE MODEL ---\n",
        "# 1. Generate ALL base forecasts using your existing recursive loop, but now for all 11 series.\n",
        "#    You need to modify your pipeline to keep store_id=0.\n",
        "# 2. Once you have the final (incoherent) predictions, format them.\n",
        "# 3. Use the reconciliation functions on this final output.\n",
        "\n",
        "# Let's assume you've run your loop and have 'final_incoherent_preds_df'\n",
        "# with columns ['ds', 'unique_id', 'y_hat'] where y_hat is the model prediction.\n",
        "\n",
        "# This gets more complex. Let's try the library's simpler, built-in way first.\n",
        "# It might give a better score due to its more robust feature creation.\n",
        "\n",
        "cross_validation_results = hf.cross_validation(\n",
        "    df=hf_df,\n",
        "    h=H, # Your forecast horizon\n",
        "    tags=tags,\n",
        "    step_size=H, # How many steps to jump for each CV fold\n",
        "    n_windows=2 # Number of cross-validation windows\n",
        ")\n",
        "\n",
        "# The result will contain reconciled forecasts.\n",
        "print(\"\\nCross-validation results with MinT reconciliation:\")\n",
        "display(cross_validation_results.head())"
      ],
      "metadata": {
        "id": "-d1zQ-tViYHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HrfD47vgtJEs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YH1sfJ4RtRPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7Axj1dIuFC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modeling with LightGBM"
      ],
      "metadata": {
        "id": "zRrNHBKX2U32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- 1. Identify Categorical Features ---\n",
        "# It's important to tell LightGBM which features are categorical.\n",
        "# This allows it to handle them more efficiently than treating them as numbers.\n",
        "categorical_features = ['store_id', 'event', 'day_of_week', 'month', 'year', 'is_weekend']\n",
        "\n",
        "# --- 2. Initialize and Train the Model ---\n",
        "# We'll start with a solid set of default parameters.\n",
        "lgbm = lgb.LGBMRegressor(\n",
        "    objective='rmse',  # Our evaluation metric\n",
        "    metric='rmse',\n",
        "    n_estimators=1000, # We'll train up to 1000 trees, but...\n",
        "    learning_rate=0.05,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,         # Use all available CPU cores\n",
        "    force_col_wise=True\n",
        ")\n",
        "\n",
        "print(\"Training LightGBM model...\")\n",
        "lgbm.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[lgb.early_stopping(10, verbose=True)], # ...early stopping will find the best number for us!\n",
        "    categorical_feature=categorical_features\n",
        ")\n",
        "\n",
        "# --- 3. Make Predictions on the Validation Set ---\n",
        "val_predictions_log = lgbm.predict(X_val)\n",
        "\n",
        "# --- 4. Inverse Transform the Predictions ---\n",
        "# This is a critical step! Our model predicted log(revenue+1).\n",
        "# We need to convert it back to actual revenue.\n",
        "val_predictions = np.expm1(val_predictions_log)\n",
        "\n",
        "# Also inverse transform the true values for comparison\n",
        "y_val_original = np.expm1(y_val)\n",
        "\n",
        "# --- 5. Evaluate the Model ---\n",
        "lgbm_rmse = np.sqrt(mean_squared_error(y_val_original, val_predictions))\n",
        "\n",
        "print(f\"\\nSeasonal Naive Baseline RMSE: 4324.50\")\n",
        "print(f\"LightGBM Model RMSE: {lgbm_rmse:.2f}\")\n",
        "\n",
        "improvement = (4324.50 - lgbm_rmse) / 4324.50 * 100\n",
        "print(f\"\\nImprovement over baseline: {improvement:.2f}%\")\n",
        "\n",
        "if lgbm_rmse < 4324.50:\n",
        "    print(\"Excellent! The LightGBM model significantly outperformed our baseline.\")\n",
        "else:\n",
        "    print(\"The LightGBM model did not beat the baseline. We may need to review features or hyperparameters.\")"
      ],
      "metadata": {
        "id": "2X0oaILfuFqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot feature importance\n",
        "lgb.plot_importance(lgbm, figsize=(12, 8), max_num_features=20)\n",
        "plt.title('LightGBM Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RpShrV8y2p-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Hyperparameter Tuning\n",
        "\n",
        "Hyperparameter Tuning with Optuna"
      ],
      "metadata": {
        "id": "51CboUaSHVKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- 1. Define the Objective Function for Optuna ---\n",
        "def objective(trial):\n",
        "    # --- Define the search space for hyperparameters ---\n",
        "    # Optuna will pick values from these ranges.\n",
        "    params = {\n",
        "        'objective': 'rmse',\n",
        "        'metric': 'rmse',\n",
        "        'n_estimators': 1000, # We still use early stopping\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', -1, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "    }\n",
        "\n",
        "    # --- Train the model with the suggested params ---\n",
        "    model = lgb.LGBMRegressor(**params)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(10, verbose=False)], # Verbose=False keeps the output clean\n",
        "        categorical_feature=categorical_features # Re-use our list of categoricals\n",
        "    )\n",
        "\n",
        "    # --- Make predictions and calculate RMSE ---\n",
        "    preds_log = model.predict(X_val)\n",
        "    preds = np.expm1(preds_log)\n",
        "    rmse = np.sqrt(mean_squared_error(np.expm1(y_val), preds))\n",
        "\n",
        "    return rmse\n",
        "\n",
        "# --- 2. Create and Run the Optuna Study ---\n",
        "# We want to 'minimize' the RMSE.\n",
        "study = optuna.create_study(direction='minimize')\n",
        "# We'll run 50 trials. For a real competition, you might do 100-200.\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "\n",
        "# --- 3. Print the Best Results ---\n",
        "print(\"\\nOptuna Study Finished!\")\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "best_trial = study.best_trial\n",
        "\n",
        "print(\"  Value (RMSE): \", best_trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ],
      "metadata": {
        "id": "nHFWffrGF72d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Train the Final Tuned Model ---\n",
        "# Get the best hyperparameters from the study\n",
        "best_params = best_trial.params\n",
        "best_params['objective'] = 'rmse'\n",
        "best_params['metric'] = 'rmse'\n",
        "best_params['random_state'] = 42\n",
        "best_params['n_estimators'] = 1000 # Use a high number, early stopping will handle it\n",
        "best_params['n_jobs'] = -1\n",
        "\n",
        "\n",
        "print(\"\\nTraining final model with best parameters...\")\n",
        "final_model = lgb.LGBMRegressor(**best_params)\n",
        "final_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[lgb.early_stopping(10, verbose=True)],\n",
        "    categorical_feature=categorical_features\n",
        ")\n",
        "\n",
        "# --- Evaluate the Final Tuned Model ---\n",
        "final_preds_log = final_model.predict(X_val)\n",
        "final_preds = np.expm1(final_preds_log)\n",
        "final_rmse = np.sqrt(mean_squared_error(y_val_original, final_preds))\n",
        "\n",
        "\n",
        "print(f\"Final Tuned LightGBM RMSE: {final_rmse:.2f}\")\n"
      ],
      "metadata": {
        "id": "oFH8Qo3tHkK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Final Prediction Pipeline"
      ],
      "metadata": {
        "id": "cHj7l7HSVjl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Retrain the final model on ALL available data ---\n",
        "# This gives the model maximum information before forecasting.\n",
        "\n",
        "# First, get the full set of features and the target\n",
        "ALL_FEATURES = [col for col in df_featured.columns if col not in [TARGET, 'store_name', 'date', 'weekday']]\n",
        "X_all = df_featured[ALL_FEATURES]\n",
        "y_all = df_featured[TARGET]\n",
        "\n",
        "# Use the best parameters found by Optuna\n",
        "final_model_for_submission = lgb.LGBMRegressor(**best_trial.params)\n",
        "\n",
        "print(\"Retraining model on all available data...\")\n",
        "final_model_for_submission.fit(X_all, y_all, categorical_feature=categorical_features)\n",
        "print(\"Model retraining complete.\")\n"
      ],
      "metadata": {
        "id": "VNAdUeE5AgFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['store_id'].unique()"
      ],
      "metadata": {
        "id": "CClQqw1rA03n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. Set up the Recursive Forecasting Loop ---\n",
        "# This is where the magic happens.\n",
        "\n",
        "# Get the last known date from our original data\n",
        "last_known_date = df['date'].max()\n",
        "# Get the last `N` days of data to compute future lags/rolling features.\n",
        "# A safe number is the max lag or window size we used. Let's use 120 days.\n",
        "history_df = df_featured.groupby('store_id').tail(120).copy()\n",
        "\n",
        "future_predictions = []\n",
        "\n",
        "print(f\"\\nStarting recursive forecast for {H} days...\")\n",
        "for i in range(H):\n",
        "    # The date we want to predict\n",
        "    predict_date = last_known_date + pd.to_timedelta(i + 1, unit='D')\n",
        "\n",
        "    # Create a placeholder row for each store for the future date\n",
        "    future_rows = []\n",
        "    for store_id in df['store_id'].unique():\n",
        "        future_rows.append({'date': predict_date, 'store_id': store_id})\n",
        "    future_df = pd.DataFrame(future_rows)\n",
        "\n",
        "    # Merge calendar events for the future date\n",
        "    future_df = pd.merge(future_df, calendar_df, on='date', how='left')\n",
        "    future_df['event'].fillna('NoEvent', inplace=True)\n",
        "\n",
        "    # Create features for this future row using our past history\n",
        "    # We combine history with the new rows to calculate lags correctly\n",
        "    combined_df = pd.concat([history_df, future_df], ignore_index=True)\n",
        "    featured_future = create_features(combined_df)\n",
        "\n",
        "    # Isolate the last 10 rows which are the ones we want to predict\n",
        "    predict_data = featured_future[featured_future['date'] == predict_date].copy()\n",
        "\n",
        "    # Select the features for prediction\n",
        "    X_predict = predict_data[ALL_FEATURES]\n",
        "\n",
        "    # Predict in log scale\n",
        "    preds_log = final_model_for_submission.predict(X_predict)\n",
        "    # Inverse transform to original scale\n",
        "    preds_revenue = np.expm1(preds_log)\n",
        "\n",
        "    # Store the predictions\n",
        "    predict_data['revenue'] = preds_revenue\n",
        "    future_predictions.append(predict_data[['date', 'store_id', 'revenue']])\n",
        "\n",
        "    # CRITICAL: Update history for the next loop iteration\n",
        "    # The predicted data becomes part of the new history\n",
        "    history_df = pd.concat([history_df, predict_data], ignore_index=True)\n",
        "\n",
        "print(\"Recursive forecast complete.\")\n",
        "\n",
        "# Combine all predictions into a single dataframe\n",
        "final_predictions_df = pd.concat(future_predictions, ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "uvn4M_HwLa47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the last known date from our original data\n",
        "last_known_date = df['date'].max()\n",
        "# Get the last `N` days of data to compute future lags/rolling features.\n",
        "# A safe number is the max lag or window size we used. Let's use 120 days.\n",
        "history_df = df_featured.groupby('store_id').tail(120).copy()\n",
        "\n",
        "future_predictions = []\n",
        "\n",
        "# The date we want to predict\n",
        "predict_date = last_known_date + pd.to_timedelta(i + 1, unit='D')\n",
        "\n",
        "# Create a placeholder row for each store for the future date\n",
        "future_rows = []\n",
        "for store_id in df['store_id'].unique():\n",
        "    future_rows.append({'date': predict_date, 'store_id': store_id})\n",
        "future_df = pd.DataFrame(future_rows)\n",
        "\n",
        "# Merge calendar events for the future date\n",
        "future_df = pd.merge(future_df, calendar_df, on='date', how='left')\n",
        "future_df['event'].fillna('NoEvent', inplace=True)\n",
        "\n",
        "# Create features for this future row using our past history\n",
        "# We combine history with the new rows to calculate lags correctly\n",
        "combined_df = pd.concat([history_df, future_df], ignore_index=True)\n",
        "featured_future = create_features(combined_df)\n",
        "featured_future\n",
        "# Isolate the last 10 rows which are the ones we want to predict\n",
        "predict_data = featured_future.tail(10)\n",
        "predict_data\n",
        "# # Select the features for prediction\n",
        "# X_predict = predict_data[ALL_FEATURES]\n",
        "\n",
        "# # Predict in log scale\n",
        "# preds_log = final_model_for_submission.predict(X_predict)\n",
        "# # Inverse transform to original scale\n",
        "# preds_revenue = np.expm1(preds_log)\n",
        "\n",
        "# # Store the predictions\n",
        "# predict_data['revenue'] = preds_revenue\n",
        "# future_predictions.append(predict_data[['date', 'store_id', 'revenue']])\n",
        "\n",
        "# # CRITICAL: Update history for the next loop iteration\n",
        "# # The predicted data becomes part of the new history\n",
        "# history_df = pd.concat([history_df, predict_data], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "RxPwIsmCAOpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions_df\n"
      ],
      "metadata": {
        "id": "cZaQtcy7E8sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Format for Submission ---\n",
        "\n",
        "# We start with our dataframe of predictions for stores 1-10\n",
        "individual_store_preds = final_predictions_df.copy()\n",
        "print(f\"Generated predictions for {individual_store_preds['store_id'].nunique()} individual stores.\")\n",
        "\n",
        "# Group by date and sum the predictions of stores 1-10 to create the forecast for store 0\n",
        "aggregate_preds = individual_store_preds.groupby('date')['revenue'].sum().reset_index()\n",
        "aggregate_preds['store_id'] = 0 # Assign the correct store ID for the aggregate\n",
        "print(\"\\nCreated aggregate predictions for 'store_id = 0' by summing individual stores.\")\n",
        "\n",
        "# We now have two dataframes: one for stores 1-10, one for store 0. Let's combine them.\n",
        "# Reorder columns in aggregate_preds to match individual_store_preds for concatenation\n",
        "aggregate_preds = aggregate_preds[['date', 'store_id', 'revenue']]\n",
        "# Append the aggregate predictions to the individual ones\n",
        "all_predictions_df = pd.concat([individual_store_preds.rename(columns={'revenue':'prediction'}), aggregate_preds.rename(columns={'revenue':'prediction'})], ignore_index=True)\n",
        "all_predictions_df\n",
        "# Now we create the 'id' column from this complete set of predictions\n",
        "all_predictions_df['id'] = all_predictions_df['store_id'].astype(str) + '_' + all_predictions_df['date'].dt.strftime('%Y%m%d')\n",
        "\n",
        "# Merge with the sample submission to ensure correct format and order\n",
        "submission_df_final = sample_submission[['id']].merge(all_predictions_df[['id', 'prediction']], on='id', how='left')\n",
        "\n",
        "submission_df_final\n",
        "# # # --- Final Sanity Check ---\n",
        "# if submission_df_final['prediction'].isnull().any():\n",
        "#     print(\"\\nWARNING: There are still NaN values in the submission file! Debugging needed.\")\n",
        "#     # Add a check to see which IDs failed to merge\n",
        "#     print(\"IDs that failed to find a match:\")\n",
        "#     print(submission_df_final[submission_df_final['prediction'].isnull()])\n",
        "# else:\n",
        "#     print(\"\\nSUCCESS! All 1,012 IDs matched and predictions are filled.\")\n",
        "\n",
        "# Save the submission file\n",
        "submission_df_final.to_csv( DATA_PATH+ 'submission_df.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission file 'submission_df.csv' created successfully!\")\n",
        "print(\"Here's a sample of the submission file:\")\n",
        "print(submission_df_final.head())"
      ],
      "metadata": {
        "id": "Z9rfZLg867CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregate_preds"
      ],
      "metadata": {
        "id": "KzY6baql8RwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Model Explainability with SHAP"
      ],
      "metadata": {
        "id": "bF4uPyoqVnvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SHAP\n",
        "!pip install shap -q\n",
        "\n",
        "import shap\n",
        "\n",
        "# --- Explain the final model trained on all data ---\n",
        "# We use a TreeExplainer for tree-based models like LightGBM\n",
        "explainer = shap.TreeExplainer(final_model_for_submission)\n",
        "# Calculate SHAP values for the validation set (as a sample)\n",
        "shap_values = explainer.shap_values(X_val)\n",
        "\n",
        "# --- Create a SHAP Summary Plot ---\n",
        "# This is like a super-powered feature importance plot.\n",
        "# It shows not only the importance but also the direction of the effect.\n",
        "shap.summary_plot(shap_values, X_val, plot_type=\"bar\", max_display=20) # Classic bar chart\n",
        "shap.summary_plot(shap_values, X_val, max_display=20) # Beeswarm plot (even better!)"
      ],
      "metadata": {
        "id": "gv9LQMBcVkRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Final Results records\n",
        "\n",
        "Date: 29.12\n",
        "*   **Best Validation RMSE:** '3614.85`\n",
        "*   **Best Leaderboard RMSE:** `7225.3`\n",
        "*   **Key Insight:** The model's feature importance plot revealed that high-level calendar features (`month`, `day_of_month`) and `store_id` were the most powerful predictors, indicating strong seasonal and store-level base patterns. Lag and rolling mean features were crucial for capturing recent dynamics."
      ],
      "metadata": {
        "id": "RBJChT2AJ28_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yfh98Jj7WgD6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}